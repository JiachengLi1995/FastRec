{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "\n",
    "from src.models import model_factory\n",
    "from src.dataloaders import dataloader_factory\n",
    "from src.datasets import dataset_factory\n",
    "from src.trainers import trainer_factory\n",
    "from src.utils.utils import *\n",
    "from src.utils.options import parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Folder created: /home/zhankui/1_engineering/Qualcomm-FastRec/demo/experiments/test_2022-01-04_26\n",
      "{'adam_epsilon': 1e-06,\n",
      " 'best_metric': 'NDCG@10',\n",
      " 'data_path': '../data/ml-1m',\n",
      " 'dataloader_code': 'sasrec',\n",
      " 'dataloader_random_seed': 0.0,\n",
      " 'dataset_code': 'item',\n",
      " 'dataset_split_seed': 98765,\n",
      " 'device': 'cpu',\n",
      " 'device_idx': '0',\n",
      " 'experiment_description': 'test',\n",
      " 'experiment_dir': 'experiments',\n",
      " 'global_epochs': 1000,\n",
      " 'local_epochs': 10,\n",
      " 'lr': 0.001,\n",
      " 'metric_ks': [5,\n",
      "               10,\n",
      "               20],\n",
      " 'model_code': 'sasrec',\n",
      " 'model_init_seed': 0,\n",
      " 'num_epochs': 100,\n",
      " 'num_gpu': 1,\n",
      " 'optimizer': 'Adam',\n",
      " 'split': 'leave_one_out',\n",
      " 'subset_size': 1000,\n",
      " 'test_batch_size': 64,\n",
      " 'test_negative_sample_size': 100,\n",
      " 'test_negative_sampler_code': 'random',\n",
      " 'test_negative_sampling_seed': 98765,\n",
      " 'train_batch_size': 64,\n",
      " 'train_negative_sample_size': 100,\n",
      " 'train_negative_sampler_code': 'random',\n",
      " 'train_negative_sampling_seed': 0,\n",
      " 'trainer_code': 'sasrec_sample',\n",
      " 'trm_att_dropout': 0.2,\n",
      " 'trm_dropout': 0.2,\n",
      " 'trm_hidden_dim': 50,\n",
      " 'trm_max_len': 50,\n",
      " 'trm_num_blocks': 2,\n",
      " 'trm_num_heads': 1,\n",
      " 'val_batch_size': 64,\n",
      " 'verbose': 10,\n",
      " 'weight_decay': 0}\n"
     ]
    }
   ],
   "source": [
    "args = parser.parse_args([])\n",
    "\n",
    "args.data_path = '../data/ml-1m' # \"../data/Beauty\"\n",
    "args.num_epochs = 100\n",
    "args.trm_max_len = 50\n",
    "\n",
    "ckpt_root = setup_train(args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset_factory(args)\n",
    "train_loader, val_loader, test_loader, dataset = dataloader_factory(args, dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch in test_loader:\n",
    "    break\n",
    "users, seqs, candidates, labels, length = batch\n",
    "\n",
    "def to_numpy(tensor):\n",
    "    return tensor.detach().cpu().numpy() if tensor.requires_grad else tensor.cpu().numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation Prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model_factory(args)\n",
    "trainer = trainer_factory(args, model, train_loader, val_loader, test_loader, ckpt_root, dataset.data)\n",
    "\n",
    "from tqdm import tqdm\n",
    "from src.utils.utils import AverageMeterSet\n",
    "\n",
    "def evaluate(session, test_loader, metric_ks, ranker):\n",
    "    average_meter_set = AverageMeterSet()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        tqdm_dataloader = tqdm(test_loader)\n",
    "        for batch_idx, batch in enumerate(tqdm_dataloader):\n",
    "            users, seqs, candidates, labels, length = batch\n",
    "            if users.size(0) != args.test_batch_size:\n",
    "                continue\n",
    "\n",
    "            ort_inputs = {'seqs': to_numpy(seqs), 'candidates': to_numpy(candidates), 'length': to_numpy(length)}\n",
    "            scores = torch.Tensor(session.run(None, ort_inputs)[0])\n",
    "\n",
    "            res = ranker(scores)\n",
    "            metrics = {}\n",
    "            for i, k in enumerate(metric_ks):\n",
    "                metrics[\"NDCG@%d\" % k] = res[2*i]\n",
    "                metrics[\"Recall@%d\" % k] = res[2*i+1]\n",
    "            metrics[\"MRR\"] = res[-3]\n",
    "            metrics[\"AUC\"] = res[-2]\n",
    "\n",
    "            for k, v in metrics.items():\n",
    "                average_meter_set.update(k, v)\n",
    "            description_metrics = ['NDCG@%d' % k for k in metric_ks[:3]] +\\\n",
    "                                    ['Recall@%d' % k for k in metric_ks[:3]] + ['MRR'] + ['AUC'] + ['loss']\n",
    "            description = 'FINAL TEST: ' + ', '.join(s + ' {:.5f}' for s in description_metrics)\n",
    "            description = description.replace('NDCG', 'N').replace('Recall', 'R').replace('MRR', 'M').replace('Jaccard', 'J')\n",
    "            description = description.format(*(average_meter_set[k].avg for k in description_metrics))\n",
    "            tqdm_dataloader.set_description(description)\n",
    "\n",
    "        average_metrics = average_meter_set.averages()\n",
    "\n",
    "        return average_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert To FP16 Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_fp16():\n",
    "\n",
    "    import onnx \n",
    "    import onnxruntime\n",
    "    import onnxmltools\n",
    "    from onnxmltools.utils.float16_converter import convert_float_to_float16\n",
    "\n",
    "    onnx_model = onnx.load(\"model.onnx\")\n",
    "    onnx.checker.check_model(onnx_model)\n",
    "\n",
    "    ort_session = onnxruntime.InferenceSession(\"model.onnx\")\n",
    "\n",
    "    # compute ONNX Runtime output prediction\n",
    "    ort_inputs = {'seqs': to_numpy(seqs), 'candidates': to_numpy(candidates), 'length': to_numpy(length)}\n",
    "    ort_outs = ort_session.run(None, ort_inputs)\n",
    "\n",
    "    onnx_model = convert_float_to_float16(onnx_model)\n",
    "    onnxmltools.utils.save_model(onnx_model, \"model_fp16.onnx\")\n",
    "\n",
    "convert_fp16()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zhankui/anaconda3/envs/bert4rec/lib/python3.6/site-packages/onnxruntime/capi/onnxruntime_inference_collection.py:353: UserWarning: Deprecation warning. This ORT build has ['CUDAExecutionProvider', 'CPUExecutionProvider'] enabled. The next release (ORT 1.10) will require explicitly setting the providers parameter (as opposed to the current behavior of providers getting set/registered by default based on the build flags) when instantiating InferenceSession.For example, onnxruntime.InferenceSession(..., providers=[\"CUDAExecutionProvider\"], ...)\n",
      "  \"based on the build flags) when instantiating InferenceSession.\"\n"
     ]
    }
   ],
   "source": [
    "import onnxruntime \n",
    "\n",
    "ort_session_fp16 = onnxruntime.InferenceSession(\"model_fp16.onnx\")\n",
    "ort_session = onnxruntime.InferenceSession(\"model.onnx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "ort_inputs = {'seqs': to_numpy(seqs), 'candidates': to_numpy(candidates), 'length': to_numpy(length)}\n",
    "ort_outs_fp16 = ort_session_fp16.run(None, ort_inputs)\n",
    "ort_outs = ort_session.run(None, ort_inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "FINAL TEST: N@5 0.36613, N@10 0.40811, N@20 0.43920, R@5 0.49102, R@10 0.62051, R@20 0.74318, M 0.35758, AUC 0.85303, loss 0.00000: 100%|██████████| 95/95 [00:00<00:00, 353.34it/s]\n",
      "FINAL TEST: N@5 0.36651, N@10 0.40864, N@20 0.43956, R@5 0.49152, R@10 0.62151, R@20 0.74352, M 0.35790, AUC 0.85300, loss 0.00000: 100%|██████████| 95/95 [00:00<00:00, 374.43it/s]\n"
     ]
    }
   ],
   "source": [
    "res_fp16 = evaluate(ort_session_fp16, test_loader, args.metric_ks, trainer.ranker)\n",
    "res = evaluate(ort_session, test_loader, args.metric_ks, trainer.ranker)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC Change: \t 1.000040911340166\n"
     ]
    }
   ],
   "source": [
    "print(f\"AUC Change: \\t {res_fp16['AUC'] / res['AUC']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert To Mixed-Precision Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_mixed():\n",
    "    import onnx \n",
    "    from onnxruntime.quantization import quantize_dynamic, QuantType\n",
    "\n",
    "    quantize_dynamic(\"model_fp16.onnx\", \"model_mixed.onnx\", weight_type=QuantType.QInt8)\n",
    "\n",
    "convert_mixed()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import onnxruntime\n",
    "\n",
    "ort_session_mixed = onnxruntime.InferenceSession(\"model_mixed.onnx\")\n",
    "ort_session = onnxruntime.InferenceSession(\"model.onnx\")\n",
    "\n",
    "ort_inputs = {'seqs': to_numpy(seqs), 'candidates': to_numpy(candidates), 'length': to_numpy(length)}\n",
    "ort_outs_mixed = ort_session_mixed.run(None, ort_inputs)\n",
    "print(ort_outs_mixed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "FINAL TEST: N@5 0.02813, N@10 0.15688, N@20 0.24542, R@5 0.04375, R@10 0.43542, R@20 0.78542, M 0.10499, AUC 0.77461, loss 0.00000: 100%|██████████| 16/16 [00:00<00:00, 143.71it/s]\n",
      "FINAL TEST: N@5 0.03037, N@10 0.16213, N@20 0.24740, R@5 0.04792, R@10 0.44688, R@20 0.78438, M 0.10727, AUC 0.77543, loss 0.00000: 100%|██████████| 16/16 [00:00<00:00, 435.79it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC Change: \t 0.9989521714670064\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "res_mixed = evaluate(ort_session_mixed, test_loader, args.metric_ks, trainer.ranker)\n",
    "res = evaluate(ort_session, test_loader, args.metric_ks, trainer.ranker)\n",
    "\n",
    "print(f\"AUC Change: \\t {res_mixed['AUC'] / res['AUC']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b924b7c73943409674542aea3e0df54fb1f044d153fc4e01dc776af20050e158"
  },
  "kernelspec": {
   "display_name": "Python 3.6.10 64-bit ('bert4rec': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
