{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Pre-processing\n",
    "### Download Beauty Dataset from [Amazon 2018](https://nijianmo.github.io/amazon/index.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5269it [00:00, 109003.41it/s]\n",
      "100%|█████████████████████████████████████| 991/991 [00:00<00:00, 443602.48it/s]\n"
     ]
    }
   ],
   "source": [
    "!python data/data_process.py --file_path data/Beauty/All_Beauty_5.json.gz --output_path data/Beauty"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Dependencies and Hyper-Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "from src.models import model_factory\n",
    "from src.dataloaders import dataloader_factory\n",
    "from src.datasets import dataset_factory\n",
    "from src.trainers import trainer_factory\n",
    "from src.utils.utils import *\n",
    "from src.utils.options import parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Folder created: /home/zhankui/FastRec/experiments/test_2021-04-01_11\n",
      "{'adam_epsilon': 1e-06,\n",
      " 'best_metric': 'NDCG@10',\n",
      " 'data_path': 'data/Beauty',\n",
      " 'dataloader_code': 'sasrec',\n",
      " 'dataloader_random_seed': 0.0,\n",
      " 'dataset_code': 'item',\n",
      " 'dataset_split_seed': 98765,\n",
      " 'device': 'cpu',\n",
      " 'device_idx': '0',\n",
      " 'experiment_description': 'test',\n",
      " 'experiment_dir': 'experiments',\n",
      " 'global_epochs': 1000,\n",
      " 'local_epochs': 10,\n",
      " 'lr': 0.001,\n",
      " 'metric_ks': [5,\n",
      "               10,\n",
      "               20],\n",
      " 'model_code': 'sasrec',\n",
      " 'model_init_seed': 0,\n",
      " 'num_epochs': 100,\n",
      " 'num_gpu': 1,\n",
      " 'optimizer': 'Adam',\n",
      " 'split': 'leave_one_out',\n",
      " 'subset_size': 1000,\n",
      " 'test_batch_size': 64,\n",
      " 'test_negative_sample_size': 100,\n",
      " 'test_negative_sampler_code': 'random',\n",
      " 'test_negative_sampling_seed': 98765,\n",
      " 'train_batch_size': 64,\n",
      " 'train_negative_sample_size': 100,\n",
      " 'train_negative_sampler_code': 'random',\n",
      " 'train_negative_sampling_seed': 0,\n",
      " 'trainer_code': 'sasrec_sample',\n",
      " 'trm_att_dropout': 0.2,\n",
      " 'trm_dropout': 0.2,\n",
      " 'trm_hidden_dim': 50,\n",
      " 'trm_max_len': 50,\n",
      " 'trm_num_blocks': 2,\n",
      " 'trm_num_heads': 1,\n",
      " 'val_batch_size': 64,\n",
      " 'verbose': 10,\n",
      " 'weight_decay': 0}\n"
     ]
    }
   ],
   "source": [
    "args = parser.parse_args([])\n",
    "\n",
    "args.data_path = 'data/Beauty'\n",
    "args.num_epochs = 100\n",
    "args.trm_max_len = 50\n",
    "\n",
    "ckpt_root = setup_train(args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset_factory(args)\n",
    "train_loader, val_loader, test_loader, dataset = dataloader_factory(args, dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Setup\n",
    "We load [SASRec Model](https://arxiv.org/abs/1808.09781) for sequential recommendation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model size: 37950\n",
      "SASRecModel(\n",
      "  (loss): BCEWithLogitsLoss()\n",
      "  (item_emb): Embedding(87, 50, padding_idx=86)\n",
      "  (pos_emb): Embedding(50, 50)\n",
      "  (emb_dropout): Dropout(p=0.2, inplace=False)\n",
      "  (attention_layernorms): ModuleList(\n",
      "    (0): LayerNorm((50,), eps=1e-08, elementwise_affine=True)\n",
      "    (1): LayerNorm((50,), eps=1e-08, elementwise_affine=True)\n",
      "  )\n",
      "  (attention_layers): ModuleList(\n",
      "    (0): MultiheadAttention(\n",
      "      (out_proj): _LinearWithBias(in_features=50, out_features=50, bias=True)\n",
      "    )\n",
      "    (1): MultiheadAttention(\n",
      "      (out_proj): _LinearWithBias(in_features=50, out_features=50, bias=True)\n",
      "    )\n",
      "  )\n",
      "  (forward_layernorms): ModuleList(\n",
      "    (0): LayerNorm((50,), eps=1e-08, elementwise_affine=True)\n",
      "    (1): LayerNorm((50,), eps=1e-08, elementwise_affine=True)\n",
      "  )\n",
      "  (forward_layers): ModuleList(\n",
      "    (0): PointWiseFeedForward(\n",
      "      (conv1): Conv1d(50, 50, kernel_size=(1,), stride=(1,))\n",
      "      (dropout1): Dropout(p=0.2, inplace=False)\n",
      "      (relu): ReLU()\n",
      "      (conv2): Conv1d(50, 50, kernel_size=(1,), stride=(1,))\n",
      "      (dropout2): Dropout(p=0.2, inplace=False)\n",
      "    )\n",
      "    (1): PointWiseFeedForward(\n",
      "      (conv1): Conv1d(50, 50, kernel_size=(1,), stride=(1,))\n",
      "      (dropout1): Dropout(p=0.2, inplace=False)\n",
      "      (relu): ReLU()\n",
      "      (conv2): Conv1d(50, 50, kernel_size=(1,), stride=(1,))\n",
      "      (dropout2): Dropout(p=0.2, inplace=False)\n",
      "    )\n",
      "  )\n",
      "  (last_layernorm): LayerNorm((50,), eps=1e-08, elementwise_affine=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = model_factory(args)\n",
    "if args.load_pretrained_weights is not None:\n",
    "    print(\"weights loading from %s ...\" % args.load_pretrained_weights)\n",
    "    model = load_pretrained_weights(model, args.load_pretrained_weights)\n",
    "print(\"Model size:\", sum(p.numel() for p in model.parameters() if p.requires_grad))\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zhankui/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/zhankui/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/zhankui/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/zhankui/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:529: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/zhankui/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:530: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/zhankui/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:535: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "TEST N@5 0.39744: , N@10 0.40122: , N@20 0.41062: , R@5 0.41006: , R@10 0.42178: , R@20 0.45908: , M 0.41026: , AUC 0.75416: , loss 0.00000: 100%|██████████| 16/16 [00:01<00:00,  9.12it/s]\n",
      "  0%|          | 0/16 [00:00<?, ?it/s]/home/zhankui/FastRec/src/trainers/base.py:83: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
      "  nn.utils.clip_grad_norm(self.model.parameters(), 5.0)\n",
      "Epoch 1, loss 1.08816 : 100%|██████████| 16/16 [00:00<00:00, 28.57it/s]\n",
      "Epoch 2, loss 0.77310 : 100%|██████████| 16/16 [00:00<00:00, 28.92it/s]\n",
      "Epoch 3, loss 0.64171 : 100%|██████████| 16/16 [00:00<00:00, 27.73it/s]\n",
      "Epoch 4, loss 0.53880 : 100%|██████████| 16/16 [00:00<00:00, 28.24it/s]\n",
      "Epoch 5, loss 0.49943 : 100%|██████████| 16/16 [00:00<00:00, 28.45it/s]\n",
      "Epoch 6, loss 0.42017 : 100%|██████████| 16/16 [00:00<00:00, 28.53it/s]\n",
      "Epoch 7, loss 0.40783 : 100%|██████████| 16/16 [00:00<00:00, 28.53it/s]\n",
      "Epoch 8, loss 0.34670 : 100%|██████████| 16/16 [00:00<00:00, 28.43it/s]\n",
      "Epoch 9, loss 0.35512 : 100%|██████████| 16/16 [00:00<00:00, 28.64it/s]\n",
      "Epoch 10, loss 0.30711 : 100%|██████████| 16/16 [00:00<00:00, 28.57it/s]\n",
      "VAL N@5 0.89581: , N@10 0.89708: , N@20 0.90011: , R@5 0.97155: , R@10 0.97546: , R@20 0.98822: , M 0.87185: , AUC 0.98789: , loss 0.00000: 100%|██████████| 16/16 [00:00<00:00, 153.60it/s]\n",
      "TEST N@5 0.65269: , N@10 0.65748: , N@20 0.65977: , R@5 0.82677: , R@10 0.84142: , R@20 0.85027: , M 0.59867: , AUC 0.88591: , loss 0.00000: 100%|██████████| 16/16 [00:00<00:00, 154.84it/s]\n",
      "Epoch 11, loss 0.36900 :   0%|          | 0/16 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Update Best NDCG@10 Model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11, loss 0.29852 : 100%|██████████| 16/16 [00:00<00:00, 28.33it/s]\n",
      "Epoch 12, loss 0.29994 : 100%|██████████| 16/16 [00:00<00:00, 28.56it/s]\n",
      "Epoch 13, loss 0.28317 : 100%|██████████| 16/16 [00:00<00:00, 28.30it/s]\n",
      "Epoch 14, loss 0.26775 : 100%|██████████| 16/16 [00:00<00:00, 28.20it/s]\n",
      "Epoch 15, loss 0.26194 : 100%|██████████| 16/16 [00:00<00:00, 27.56it/s]\n",
      "Epoch 16, loss 0.25033 : 100%|██████████| 16/16 [00:00<00:00, 28.64it/s]\n",
      "Epoch 17, loss 0.25845 : 100%|██████████| 16/16 [00:00<00:00, 28.61it/s]\n",
      "Epoch 18, loss 0.22764 : 100%|██████████| 16/16 [00:00<00:00, 28.42it/s]\n",
      "Epoch 19, loss 0.24128 : 100%|██████████| 16/16 [00:00<00:00, 28.55it/s]\n",
      "Epoch 20, loss 0.24461 : 100%|██████████| 16/16 [00:00<00:00, 28.62it/s]\n",
      "VAL N@5 0.94493: , N@10 0.94671: , N@20 0.94891: , R@5 0.97461: , R@10 0.98047: , R@20 0.98926: , M 0.93631: , AUC 0.99182: , loss 0.00000: 100%|██████████| 16/16 [00:00<00:00, 144.87it/s]\n",
      "TEST N@5 0.79299: , N@10 0.79483: , N@20 0.79597: , R@5 0.82945: , R@10 0.83537: , R@20 0.84032: , M 0.78392: , AUC 0.88682: , loss 0.00000: 100%|██████████| 16/16 [00:00<00:00, 144.65it/s]\n",
      "Epoch 21, loss 0.23461 :   0%|          | 0/16 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Update Best NDCG@10 Model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 21, loss 0.23665 : 100%|██████████| 16/16 [00:00<00:00, 26.43it/s]\n",
      "Epoch 22, loss 0.21018 : 100%|██████████| 16/16 [00:00<00:00, 27.93it/s]\n",
      "Epoch 23, loss 0.21250 : 100%|██████████| 16/16 [00:00<00:00, 28.12it/s]\n",
      "Epoch 24, loss 0.21088 : 100%|██████████| 16/16 [00:00<00:00, 28.02it/s]\n",
      "Epoch 25, loss 0.20638 : 100%|██████████| 16/16 [00:00<00:00, 28.32it/s]\n",
      "Epoch 26, loss 0.18754 : 100%|██████████| 16/16 [00:00<00:00, 27.49it/s]\n",
      "Epoch 27, loss 0.17718 : 100%|██████████| 16/16 [00:00<00:00, 28.50it/s]\n",
      "Epoch 28, loss 0.18986 : 100%|██████████| 16/16 [00:00<00:00, 28.39it/s]\n",
      "Epoch 29, loss 0.18707 : 100%|██████████| 16/16 [00:00<00:00, 26.87it/s]\n",
      "Epoch 30, loss 0.17356 : 100%|██████████| 16/16 [00:00<00:00, 24.83it/s]\n",
      "VAL N@5 0.94405: , N@10 0.94662: , N@20 0.94927: , R@5 0.97070: , R@10 0.97852: , R@20 0.98926: , M 0.93698: , AUC 0.99163: , loss 0.00000: 100%|██████████| 16/16 [00:00<00:00, 148.45it/s]\n",
      "TEST N@5 0.79827: , N@10 0.79960: , N@20 0.80191: , R@5 0.83550: , R@10 0.83940: , R@20 0.84917: , M 0.78873: , AUC 0.88113: , loss 0.00000: 100%|██████████| 16/16 [00:00<00:00, 150.02it/s]\n",
      "Epoch 31, loss 0.18139 : 100%|██████████| 16/16 [00:00<00:00, 27.63it/s]\n",
      "Epoch 32, loss 0.17710 : 100%|██████████| 16/16 [00:00<00:00, 28.27it/s]\n",
      "Epoch 33, loss 0.16955 : 100%|██████████| 16/16 [00:00<00:00, 26.62it/s]\n",
      "Epoch 34, loss 0.18506 : 100%|██████████| 16/16 [00:00<00:00, 28.12it/s]\n",
      "Epoch 35, loss 0.17331 : 100%|██████████| 16/16 [00:00<00:00, 28.19it/s]\n",
      "Epoch 36, loss 0.17055 : 100%|██████████| 16/16 [00:00<00:00, 28.17it/s]\n",
      "Epoch 37, loss 0.15319 : 100%|██████████| 16/16 [00:00<00:00, 28.05it/s]\n",
      "Epoch 38, loss 0.15716 : 100%|██████████| 16/16 [00:00<00:00, 28.53it/s]\n",
      "Epoch 39, loss 0.15467 : 100%|██████████| 16/16 [00:00<00:00, 28.25it/s]\n",
      "Epoch 40, loss 0.18526 : 100%|██████████| 16/16 [00:00<00:00, 28.33it/s]\n",
      "VAL N@5 0.94074: , N@10 0.94319: , N@20 0.94492: , R@5 0.97058: , R@10 0.97845: , R@20 0.98529: , M 0.93235: , AUC 0.99098: , loss 0.00000: 100%|██████████| 16/16 [00:00<00:00, 133.88it/s]\n",
      "TEST N@5 0.80164: , N@10 0.80261: , N@20 0.80381: , R@5 0.84063: , R@10 0.84356: , R@20 0.84844: , M 0.79106: , AUC 0.87683: , loss 0.00000: 100%|██████████| 16/16 [00:00<00:00, 120.26it/s]\n",
      "Epoch 41, loss 0.17682 : 100%|██████████| 16/16 [00:00<00:00, 27.62it/s]\n",
      "Epoch 42, loss 0.17581 : 100%|██████████| 16/16 [00:00<00:00, 27.14it/s]\n",
      "Epoch 43, loss 0.16003 : 100%|██████████| 16/16 [00:00<00:00, 28.52it/s]\n",
      "Epoch 44, loss 0.16618 : 100%|██████████| 16/16 [00:00<00:00, 28.23it/s]\n",
      "Epoch 45, loss 0.16712 : 100%|██████████| 16/16 [00:00<00:00, 28.20it/s]\n",
      "Epoch 46, loss 0.14840 : 100%|██████████| 16/16 [00:00<00:00, 28.60it/s]\n",
      "Epoch 47, loss 0.16466 : 100%|██████████| 16/16 [00:00<00:00, 28.50it/s]\n",
      "Epoch 48, loss 0.14444 : 100%|██████████| 16/16 [00:00<00:00, 27.72it/s]\n",
      "Epoch 49, loss 0.13549 : 100%|██████████| 16/16 [00:00<00:00, 27.19it/s]\n",
      "Epoch 50, loss 0.16089 : 100%|██████████| 16/16 [00:00<00:00, 26.64it/s]\n",
      "VAL N@5 0.93433: , N@10 0.93804: , N@20 0.94023: , R@5 0.96569: , R@10 0.97741: , R@20 0.98627: , M 0.92595: , AUC 0.99003: , loss 0.00000: 100%|██████████| 16/16 [00:00<00:00, 150.14it/s]\n",
      "TEST N@5 0.79441: , N@10 0.79578: , N@20 0.79743: , R@5 0.83439: , R@10 0.83836: , R@20 0.84520: , M 0.78371: , AUC 0.87329: , loss 0.00000: 100%|██████████| 16/16 [00:00<00:00, 148.70it/s]\n",
      "Epoch 51, loss 0.14710 : 100%|██████████| 16/16 [00:00<00:00, 28.57it/s]\n",
      "Epoch 52, loss 0.15158 : 100%|██████████| 16/16 [00:00<00:00, 28.35it/s]\n",
      "Epoch 53, loss 0.13646 : 100%|██████████| 16/16 [00:00<00:00, 27.47it/s]\n",
      "Epoch 54, loss 0.15746 : 100%|██████████| 16/16 [00:00<00:00, 27.61it/s]\n",
      "Epoch 55, loss 0.13812 : 100%|██████████| 16/16 [00:00<00:00, 28.24it/s]\n",
      "Epoch 56, loss 0.14171 : 100%|██████████| 16/16 [00:00<00:00, 28.01it/s]\n",
      "Epoch 57, loss 0.15430 : 100%|██████████| 16/16 [00:00<00:00, 28.37it/s]\n",
      "Epoch 58, loss 0.15160 : 100%|██████████| 16/16 [00:00<00:00, 28.51it/s]\n",
      "Epoch 59, loss 0.15890 : 100%|██████████| 16/16 [00:00<00:00, 27.02it/s]\n",
      "Epoch 60, loss 0.15388 : 100%|██████████| 16/16 [00:00<00:00, 27.06it/s]\n",
      "VAL N@5 0.93955: , N@10 0.94243: , N@20 0.94337: , R@5 0.96966: , R@10 0.97845: , R@20 0.98236: , M 0.93115: , AUC 0.98986: , loss 0.00000: 100%|██████████| 16/16 [00:00<00:00, 144.59it/s]\n",
      "TEST N@5 0.79614: , N@10 0.79837: , N@20 0.79906: , R@5 0.83770: , R@10 0.84454: , R@20 0.84747: , M 0.78495: , AUC 0.87771: , loss 0.00000: 100%|██████████| 16/16 [00:00<00:00, 133.35it/s]\n",
      "Epoch 61, loss 0.15057 : 100%|██████████| 16/16 [00:00<00:00, 27.40it/s]\n",
      "Epoch 62, loss 0.13818 : 100%|██████████| 16/16 [00:00<00:00, 25.00it/s]\n",
      "Epoch 63, loss 0.15926 : 100%|██████████| 16/16 [00:00<00:00, 25.07it/s]\n",
      "Epoch 64, loss 0.18600 : 100%|██████████| 16/16 [00:00<00:00, 28.23it/s]\n",
      "Epoch 65, loss 0.13906 : 100%|██████████| 16/16 [00:00<00:00, 27.53it/s]\n",
      "Epoch 66, loss 0.13266 : 100%|██████████| 16/16 [00:00<00:00, 28.36it/s]\n",
      "Epoch 67, loss 0.13090 : 100%|██████████| 16/16 [00:00<00:00, 26.37it/s]\n",
      "Epoch 68, loss 0.14759 : 100%|██████████| 16/16 [00:00<00:00, 27.68it/s]\n",
      "Epoch 69, loss 0.12032 : 100%|██████████| 16/16 [00:00<00:00, 28.14it/s]\n",
      "Epoch 70, loss 0.13851 : 100%|██████████| 16/16 [00:00<00:00, 28.40it/s]\n",
      "VAL N@5 0.93867: , N@10 0.94176: , N@20 0.94276: , R@5 0.96875: , R@10 0.97852: , R@20 0.98242: , M 0.93027: , AUC 0.99006: , loss 0.00000: 100%|██████████| 16/16 [00:00<00:00, 146.11it/s]\n",
      "TEST N@5 0.80259: , N@10 0.80470: , N@20 0.80550: , R@5 0.83679: , R@10 0.84362: , R@20 0.84655: , M 0.79400: , AUC 0.87592: , loss 0.00000: 100%|██████████| 16/16 [00:00<00:00, 143.98it/s]\n",
      "Epoch 71, loss 0.15165 : 100%|██████████| 16/16 [00:00<00:00, 27.88it/s]\n",
      "Epoch 72, loss 0.13119 : 100%|██████████| 16/16 [00:00<00:00, 28.05it/s]\n",
      "Epoch 73, loss 0.13419 : 100%|██████████| 16/16 [00:00<00:00, 28.05it/s]\n",
      "Epoch 74, loss 0.12905 : 100%|██████████| 16/16 [00:00<00:00, 28.54it/s]\n",
      "Epoch 75, loss 0.15247 : 100%|██████████| 16/16 [00:00<00:00, 27.81it/s]\n",
      "Epoch 76, loss 0.14683 : 100%|██████████| 16/16 [00:00<00:00, 28.73it/s]\n",
      "Epoch 77, loss 0.11752 : 100%|██████████| 16/16 [00:00<00:00, 28.51it/s]\n",
      "Epoch 78, loss 0.13577 : 100%|██████████| 16/16 [00:00<00:00, 27.99it/s]\n",
      "Epoch 79, loss 0.15029 : 100%|██████████| 16/16 [00:00<00:00, 28.41it/s]\n",
      "Epoch 80, loss 0.12671 : 100%|██████████| 16/16 [00:00<00:00, 27.98it/s]\n",
      "VAL N@5 0.93809: , N@10 0.94020: , N@20 0.94233: , R@5 0.96667: , R@10 0.97351: , R@20 0.98138: , M 0.93030: , AUC 0.98940: , loss 0.00000: 100%|██████████| 16/16 [00:00<00:00, 145.30it/s]\n",
      "TEST N@5 0.79875: , N@10 0.80137: , N@20 0.80233: , R@5 0.83666: , R@10 0.84447: , R@20 0.84838: , M 0.78906: , AUC 0.87387: , loss 0.00000: 100%|██████████| 16/16 [00:00<00:00, 146.32it/s]\n",
      "Epoch 81, loss 0.13583 : 100%|██████████| 16/16 [00:00<00:00, 28.42it/s]\n",
      "Epoch 82, loss 0.12474 : 100%|██████████| 16/16 [00:00<00:00, 28.20it/s]\n",
      "Epoch 83, loss 0.12194 : 100%|██████████| 16/16 [00:00<00:00, 27.47it/s]\n",
      "Epoch 84, loss 0.13221 : 100%|██████████| 16/16 [00:00<00:00, 28.19it/s]\n",
      "Epoch 85, loss 0.14203 : 100%|██████████| 16/16 [00:00<00:00, 26.44it/s]\n",
      "Epoch 86, loss 0.12758 : 100%|██████████| 16/16 [00:00<00:00, 26.74it/s]\n",
      "Epoch 87, loss 0.12195 : 100%|██████████| 16/16 [00:00<00:00, 28.19it/s]\n",
      "Epoch 88, loss 0.13452 : 100%|██████████| 16/16 [00:00<00:00, 28.56it/s]\n",
      "Epoch 89, loss 0.14724 : 100%|██████████| 16/16 [00:00<00:00, 28.43it/s]\n",
      "Epoch 90, loss 0.13911 : 100%|██████████| 16/16 [00:00<00:00, 28.61it/s]\n",
      "VAL N@5 0.93402: , N@10 0.93654: , N@20 0.93962: , R@5 0.96478: , R@10 0.97259: , R@20 0.98431: , M 0.92581: , AUC 0.98909: , loss 0.00000: 100%|██████████| 16/16 [00:00<00:00, 132.41it/s]\n",
      "TEST N@5 0.78511: , N@10 0.78825: , N@20 0.78923: , R@5 0.82951: , R@10 0.83928: , R@20 0.84318: , M 0.77333: , AUC 0.86875: , loss 0.00000: 100%|██████████| 16/16 [00:00<00:00, 143.49it/s]\n",
      "Epoch 91, loss 0.14282 : 100%|██████████| 16/16 [00:00<00:00, 28.16it/s]\n",
      "Epoch 92, loss 0.14277 : 100%|██████████| 16/16 [00:00<00:00, 26.33it/s]\n",
      "Epoch 93, loss 0.13798 : 100%|██████████| 16/16 [00:00<00:00, 28.65it/s]\n",
      "Epoch 94, loss 0.10544 : 100%|██████████| 16/16 [00:00<00:00, 26.76it/s]\n",
      "Epoch 95, loss 0.14653 : 100%|██████████| 16/16 [00:00<00:00, 27.95it/s]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 96, loss 0.13053 : 100%|██████████| 16/16 [00:00<00:00, 25.32it/s]\n",
      "Epoch 97, loss 0.13217 : 100%|██████████| 16/16 [00:00<00:00, 25.64it/s]\n",
      "Epoch 98, loss 0.13432 : 100%|██████████| 16/16 [00:00<00:00, 28.46it/s]\n",
      "Epoch 99, loss 0.13803 : 100%|██████████| 16/16 [00:00<00:00, 28.18it/s]\n",
      "Epoch 100, loss 0.11943 : 100%|██████████| 16/16 [00:00<00:00, 24.14it/s]\n",
      "VAL N@5 0.92947: , N@10 0.93195: , N@20 0.93590: , R@5 0.96472: , R@10 0.97253: , R@20 0.98822: , M 0.91966: , AUC 0.98911: , loss 0.00000: 100%|██████████| 16/16 [00:00<00:00, 148.62it/s]\n",
      "TEST N@5 0.79006: , N@10 0.79346: , N@20 0.79557: , R@5 0.83471: , R@10 0.84545: , R@20 0.85424: , M 0.77825: , AUC 0.87558: , loss 0.00000: 100%|██████████| 16/16 [00:00<00:00, 140.72it/s]\n"
     ]
    }
   ],
   "source": [
    "trainer = trainer_factory(args, model, train_loader, val_loader, test_loader, ckpt_root, dataset.data)\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Test and Saving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "FINAL TEST: N@5 0.79260, N@10 0.79445, N@20 0.79535, R@5 0.83049, R@10 0.83641, R@20 0.84032, M 0.78300, AUC 0.88661, loss 0.00000: 100%|██████████| 16/16 [00:00<00:00, 144.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test best model with test set!\n",
      "{'NDCG@5': 0.7926026657223701, 'Recall@5': 0.8304876498878002, 'NDCG@10': 0.7944467924535275, 'Recall@10': 0.8364100307226181, 'NDCG@20': 0.7953473031520844, 'Recall@20': 0.8403162807226181, 'MRR': 0.7830023355782032, 'AUC': 0.886611957103014}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# model testing and saving\n",
    "trainer.test()\n",
    "trainer.logger_service.complete({'state_dict': (trainer._create_state_dict())})\n",
    "trainer.writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Serving\n",
    "We deploy a model with [faiss](https://github.com/facebookresearch/faiss) to accelerate the maximal inner product search. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import faiss\n",
    "\n",
    "model = model_factory(args)\n",
    "model = load_pretrained_weights(model, os.path.join(ckpt_root, 'models', 'best_acc_model.pth'))\n",
    "model = model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "d = args.trm_hidden_dim                                 # dimension\n",
    "nb = model.item_emb.weight.size(0)                      # item pool size\n",
    "xb = model.item_emb.weight.data.cpu().numpy()           # item embeddings table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = faiss.index_factory(d, 'HNSW32', faiss.METRIC_INNER_PRODUCT)   # build the index\n",
    "index.add(xb)                                                          # add vectors to the index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Input Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.LongTensor([dataset.train[0]])                # input example\n",
    "l = torch.LongTensor([len(dataset.train[0])-1])         # length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "xq = model(x, length=l, mode='serving').detach().cpu()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exact Search via Naive Matmul"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([63, 29, 28, 61, 36,  6, 49, 62, 12, 39])\n"
     ]
    }
   ],
   "source": [
    "k = 10\n",
    "\n",
    "V, I = torch.topk((torch.Tensor(xb) @ xq.squeeze()), k)\n",
    "print(I)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Approximate Search Via Faiss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[63 29 28 36 61  6 62 12 49 39]]\n"
     ]
    }
   ],
   "source": [
    "V, I = index.search(xq.numpy(), k)\n",
    "print(I)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
